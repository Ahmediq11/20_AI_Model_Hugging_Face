{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö 1. Installing Essential Libraries\n",
        "\n",
        "First things first, we need to install the necessary Python libraries to run our code. This cell uses `pip` to install the latest versions of the packages we need from the Python Package Index (PyPI).\n",
        "\n",
        "- **`transformers`**: This is the main library from Hugging Face. It gives us access to thousands of pre-trained models and the powerful `pipeline` API for easy inference.\n",
        "- **`sentencepiece`**: An unsupervised text tokenizer and detokenizer, which is a required dependency for many modern NLP models.\n",
        "- **`sacremoses`**: Another key dependency that provides tokenization and other text-processing utilities used by the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "Hugging Face models can be several gigabytes in size. When you use a model, the library downloads it to a default cache folder. This code block lets us define a custom location for this cache.\n",
        "\n",
        "By setting the `HF_HOME` environment variable, we tell the `transformers` library to save all models to our specified folder. This is a great way to manage storage and keep your projects organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 3. Importing Necessary Modules\n",
        "\n",
        "With our environment set up, we now import the specific functions we'll be using.\n",
        "\n",
        "- **`pipeline` from `transformers`**: This is the high-level API that simplifies using models for various tasks. It handles all the complex background steps for us.\n",
        "- **`pandas` as `pd`**: A powerful data analysis library. Although it's not used in this particular script's final output, it's a standard import in data science and NLP workflows for viewing data in tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåç 4. Performing Machine Translation\n",
        "\n",
        "Here, we'll use a pre-trained model to perform **Machine Translation**, converting a piece of text from one language to another.\n",
        "\n",
        "1.  **Defining the Text**: We start with a sample text in English that we wish to translate.\n",
        "\n",
        "2.  **Creating the Translation Pipeline**: We initialize the `pipeline` for a translation task. The task name `\"translation_en_to_ar\"` is a convenient shortcut that tells the pipeline we want to translate from English (`en`) to Arabic (`ar`).\n",
        "\n",
        "3.  **Model Selection**: We use the model `\"Helsinki-NLP/opus-mt-en-ar\"`. This is part of the Opus-MT project by the University of Helsinki, which provides highly optimized and accurate translation models for a vast number of language pairs.\n",
        "\n",
        "4.  **Generating the Translation**: We simply pass our English `text` to the `translator` pipeline. It processes the text and returns the translated version.\n",
        "\n",
        "5.  **Displaying the Output**: The result is a list containing a dictionary. The translated text is stored under the `\"translation_text\"` key, which we display directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "text = \"\"\"\n",
        "Dear Amazon, last week I ordered an Optimus Prime action figure from your\n",
        "online store in India. Unfortunately when I opened the package, I discovered to\n",
        "my horror that I had been sent an action figure of Megatron instead!\n",
        "\"\"\"\n",
        "\n",
        "model = \"Helsinki-NLP/opus-mt-mul-en\"\n",
        "model = \"google-t5/t5-base\"\n",
        "model = \"Helsinki-NLP/opus-mt-en-ar\"\n",
        "\n",
        "translator = pipeline(\"translation_en_to_ar\", model=model)\n",
        "\n",
        "outputs = translator(text)\n",
        "outputs"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
