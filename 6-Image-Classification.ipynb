{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö 1. Installing Essential Libraries\n",
        "\n",
        "As always, we begin by installing the necessary libraries. The `transformers` library is not just for text; it's a multimodal library that also provides powerful models for computer vision and audio tasks.\n",
        "\n",
        "- **`transformers`**: The main library from Hugging Face, providing the `pipeline` API and access to thousands of pre-trained models, including vision models.\n",
        "- **`sentencepiece` & `sacremoses`**: While these are primarily for text tokenization, it's good practice to install them as they are common dependencies in the Hugging Face ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "Hugging Face models, including vision models, can be large. This code block sets a custom directory for the library to download and cache these models.\n",
        "\n",
        "By setting the `HF_HOME` environment variable, we can better manage our disk space and keep all downloaded model assets in a single, organized location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 3. Importing Computer Vision and Helper Libraries\n",
        "\n",
        "For this task, we need a few more libraries to handle images and web requests.\n",
        "\n",
        "- **`Image` from `PIL`**: The Python Imaging Library (Pillow) is the standard library for opening, manipulating, and saving many different image file formats in Python.\n",
        "- **`requests`**: A simple and elegant HTTP library for Python, which we'll use to download an image from a URL.\n",
        "- **`pipeline` from `transformers`**: Our go-to high-level API for running models on inference tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñºÔ∏è 4. Fetching and Loading an Image\n",
        "\n",
        "Before we can classify an image, we need to get the image itself. This code block fetches an image from a URL and loads it into a format that our pipeline can understand.\n",
        "\n",
        "1.  We define the `url` of the image we want to use.\n",
        "2.  `requests.get(url, stream=True)` sends a request to the URL. `stream=True` is an efficient way to handle large files, as it doesn't load the entire content into memory at once.\n",
        "3.  `.raw` gives us the raw byte stream of the image data.\n",
        "4.  `Image.open()` takes this raw stream and opens it as a Pillow `Image` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1920,f_auto/DCTM_Penguin_UK_DK_AL697473_RGB_PNG_namnse.jpg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üëÅÔ∏è 5. Creating the Image Classification Pipeline\n",
        "\n",
        "Now we create the pipeline for our task. We simply specify `\"image-classification\"`.\n",
        "\n",
        "A key feature of the `pipeline` API is its use of sensible defaults. Since we haven't specified a particular model, the pipeline will automatically download and use the default model for image classification, which is typically **`google/vit-base-patch16-224`**, a powerful Vision Transformer (ViT) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["classifier = pipeline(\"image-classification\")"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 6. Classifying the Image and Viewing Results\n",
        "\n",
        "This is the final step where we perform the classification.\n",
        "\n",
        "We pass our loaded `image` object directly to the `classifier`. The pipeline handles all the necessary preprocessing (like resizing and normalizing the image), feeds it to the model, and then translates the model's output into human-readable labels.\n",
        "\n",
        "The output is a list of dictionaries, each containing a predicted `label` for the image and a `score` representing the model's confidence in that prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["outputs = classifier(image)\n", "outputs"]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
