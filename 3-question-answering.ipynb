{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š 1. Installing Essential Libraries\n",
        "\n",
        "Before diving into our NLP task, we need to set up our environment by installing the necessary Python packages. This cell handles the installation of key libraries required for working with Hugging Face models.\n",
        "\n",
        "- **`transformers`**: This is the main library from Hugging Face. It gives us access to thousands of pre-trained models and the powerful `pipeline` API, which simplifies using these models.\n",
        "- **`sentencepiece`**: A tokenizer and detokenizer library. It's a dependency for many modern transformer models, used to convert text into a format the model can process.\n",
        "- **`sacremoses`**: Another dependency used for text tokenization, which is a fundamental step in any NLP pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‚ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "Hugging Face models can be quite large, and the library downloads them to a local cache folder the first time you use them. This code block is a handy way to control where these large model files are stored.\n",
        "\n",
        "By setting the `HF_HOME` environment variable, we instruct the `transformers` library to use our specified folder (`X:\\...\\models`). This helps in managing disk space and keeping project-related files organized in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ 3. Importing Necessary Modules\n",
        "\n",
        "Now, we import the specific components we'll use from our installed libraries.\n",
        "\n",
        "- **`pipeline` from `transformers`**: The high-level API that allows us to perform complex NLP tasks in just a few lines of code. It handles all the background work for us.\n",
        "- **`pandas` as `pd`**: A powerful library for data manipulation and analysis. We will use it to display the model's output neatly in a table (DataFrame)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤” 4. Performing Question Answering\n",
        "\n",
        "In this section, we'll perform **Extractive Question Answering**. The goal is to ask a model a question and have it find the answer within a given piece of text (the context).\n",
        "\n",
        "1.  **Defining Context and Question**: We first define the `text` (our context) and the `question` we want to ask about it.\n",
        "\n",
        "2.  **Model Selection**: We choose a model fine-tuned for this task. `\"deepset/roberta-base-squad2\"` is a RoBERTa model that has been trained on the SQuAD 2.0 dataset, a benchmark for question answering.\n",
        "\n",
        "3.  **Creating the Pipeline**: We initialize the `pipeline` for `\"question-answering\"`. This specific pipeline is designed to take a `question` and a `context` as input.\n",
        "\n",
        "4.  **Getting the Answer**: We call the `reader` pipeline, passing our question and context. The model reads the context and extracts the span of text that it believes is the most likely answer.\n",
        "\n",
        "5.  **Displaying the Result**: The output is a dictionary containing the `answer`, a confidence `score`, and its start/end positions. We wrap it in a list `[outputs]` and convert it to a Pandas DataFrame for a clean, easy-to-read presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "text = \"\"\"\n",
        "Dear Amazon, last week I ordered an Optimus Prime action figure from your\n",
        "online store in India. Unfortunately when I opened the package, I discovered to\n",
        "my horror that I had been sent an action figure of Megatron instead!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = \"deepset/gelectra-large-germanquad\"\n",
        "model = \"mrm8488/bert-tiny-5-finetuned-squadv2\"\n",
        "model = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "reader = pipeline(\"question-answering\", model=model, device=\"cuda\")\n",
        "question = \"from where did I placed order?\"\n",
        "\n",
        "outputs = reader(question=question, context=text)\n",
        "pd.DataFrame([outputs])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
