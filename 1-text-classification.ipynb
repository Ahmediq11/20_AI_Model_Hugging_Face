{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“š 1. Installing Essential Libraries\n",
        "\n",
        "Before we can start working with powerful language models, we need to install the necessary libraries. This cell handles the installation of key packages from the Python Package Index (PyPI).\n",
        "\n",
        "- **`transformers`**: This is the core library from Hugging Face. It provides the tools and models we need for various NLP tasks, including the `pipeline` API which makes it incredibly simple to use pre-trained models.\n",
        "- **`sentencepiece`**: This is a tokenizer/detokenizer library, required by many modern transformer models (like T5, ALBERT, XLNet) to properly process text.\n",
        "- **`sacremoses`**: Another dependency for tokenization, this library is often used as an underlying tool by the `transformers` library for handling text in different languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‚ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "Hugging Face models can be quite large (sometimes several gigabytes!). When you use a model for the first time, the `transformers` library downloads it and saves it to a cache directory. By default, this is usually in your user home folder.\n",
        "\n",
        "This code block is useful for managing storage. It sets an environment variable `HF_HOME` to a custom path (`X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models`). This tells Hugging Face to download and store all models in this specific folder. It's a great way to keep your main drive clean and organize your model files, especially if you're working with many different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ 3. Importing Necessary Modules\n",
        "\n",
        "Now that our environment is set up, we import the specific tools we'll use in this script.\n",
        "\n",
        "- **`pipeline` from `transformers`**: This is a high-level, easy-to-use API that simplifies the process of using models for inference. It handles all the complex background steps like tokenization, feeding data to the model, and decoding the output.\n",
        "- **`pandas` as `pd`**: Pandas is a powerful data analysis and manipulation library. We'll use it here to display the model's output in a clean, readable table called a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ¨ 4. Performing Text Classification for Emotion Detection\n",
        "\n",
        "This is where the magic happens! We'll use the `pipeline` to classify the emotion of a given text.\n",
        "\n",
        "1.  **Model Selection**: We specify the model we want to use from the Hugging Face Hub. Here, we're using `\"SamLowe/roberta-base-go_emotions\"`. This is a RoBERTa model that has been specifically fine-tuned on a dataset of emotions, making it excellent for this task.\n",
        "\n",
        "2.  **Creating the Pipeline**: We instantiate the `pipeline` for `\"text-classification\"`. We pass our chosen model and specify `device=\"cuda\"` to instruct the pipeline to use a GPU for computation. Using a GPU significantly speeds up the process. If you don't have a GPU or CUDA is not configured, you can remove this argument or set it to `device=-1` to use the CPU.\n",
        "\n",
        "3.  **Inference**: We provide a sample sentence, `text = \"wow! we have come across this far\"`, to our `classifier`.\n",
        "\n",
        "4.  **Displaying Results**: The classifier returns a list of dictionaries, where each dictionary contains a predicted `label` (the emotion) and a `score` (the model's confidence). We wrap this output in a `pd.DataFrame()` to present the results in a clear and organized table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e98223da122848b5bc95de6335347eb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dream\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dream\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a3da7ac644f403c947cf8c78b694778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7949b83e8d934f89b876d9df41ebc003",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e726b76481143a58a6c4d74bf54b92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d02fd6ee4864f248aec079a1aa9ac29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a33617db46094b0c8a6a5f1c09179f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42307bcd3afc482681b1f1710410cc17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>surprise</td>\n",
              "      <td>0.630752</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label     score\n",
              "0  surprise  0.630752"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = \"SamLowe/roberta-base-go_emotions\"\n",
        "model = \"tabularisai/multilingual-sentiment-analysis\"\n",
        "model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model = model, device=\"cuda\")\n",
        "\n",
        "text = \"wow! we have come across this far\"\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
