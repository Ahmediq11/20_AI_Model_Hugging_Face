{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö 1. Installing Essential Libraries\n",
        "\n",
        "Let's start by installing the necessary Python libraries. This step ensures we have all the tools we need to work with Hugging Face's state-of-the-art models.\n",
        "\n",
        "- **`transformers`**: This is the core library from Hugging Face. It provides the `pipeline` API, which allows for easy access to pre-trained models for a wide range of tasks.\n",
        "- **`sentencepiece`**: A tokenization library that is a dependency for many modern transformer models. It's essential for converting text into a format that the model can understand.\n",
        "- **`sacremoses`**: Another dependency that provides tokenization and other text-handling utilities, often used by the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U sentencepiece\n",
        "!pip install -U sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "When you use a Hugging Face model, it's downloaded and saved to a cache directory on your machine. This can consume a lot of space in the default location (your user home folder).\n",
        "\n",
        "This code block sets an environment variable, `HF_HOME`, to a custom path. This tells Hugging Face to download and store all models in our specified folder. It's a great practice for keeping your projects and large model files organized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 3. Importing Necessary Modules\n",
        "\n",
        "Here, we import the specific components we'll need from the libraries we installed.\n",
        "\n",
        "- **`pipeline` from `transformers`**: This is the high-level function that simplifies using models for inference. It handles all the complex steps like tokenization, model prediction, and decoding the output.\n",
        "- **`pandas` as `pd`**: While not used in the final step of this specific notebook, importing Pandas is a common practice in data science workflows for data manipulation and display. It's good to have it ready!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 4. Performing Text Summarization\n",
        "\n",
        "This is the main event of our script: **Abstractive Text Summarization**. This task involves creating a short, coherent summary of a longer text. Unlike extractive summarization (which just picks important sentences), abstractive summarization generates new sentences that capture the core meaning of the original document.\n",
        "\n",
        "1.  **Defining the Text**: We start with a block of text‚Äîin this case, a customer complaint‚Äîthat we want to summarize.\n",
        "\n",
        "2.  **Model Selection**: We choose a model specifically trained for summarization. `\"ARTeLab/it5-summarization-mlsum\"` is a T5-based model fine-tuned on a multilingual summarization dataset, making it suitable for this task.\n",
        "\n",
        "3.  **Creating the Pipeline**: We initialize the `pipeline` for the `\"summarization\"` task, passing our chosen model and specifying `device=\"cuda\"` to use a GPU for faster processing.\n",
        "\n",
        "4.  **Generating the Summary**: We pass our `text` to the `summarizer` pipeline. The model processes the input and generates a concise summary.\n",
        "\n",
        "5.  **Displaying the Output**: The pipeline returns a list containing a dictionary. The summarized text is found under the `\"summary_text\"` key. We simply print this output to see the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "text = \"\"\"\n",
        "Dear Amazon, last week I ordered an Optimus Prime action figure from your\n",
        "online store in India. Unfortunately when I opened the package, I discovered to\n",
        "my horror that I had been sent an action figure of Megatron instead!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = \"facebook/bart-large-cnn\"\n",
        "model = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "model = \"ARTeLab/it5-summarization-mlsum\"\n",
        "\n",
        "summarizer = pipeline(\"summarization\" , model=model, device=\"cuda\")\n",
        "\n",
        "outputs = summarizer(text)\n",
        "outputs"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
