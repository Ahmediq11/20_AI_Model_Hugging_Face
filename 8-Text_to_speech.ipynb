{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö 1. Installing Required Libraries\n",
        "\n",
        "This notebook focuses on an audio task, so we need to install the necessary libraries to handle audio data in addition to the standard `transformers` package.\n",
        "\n",
        "- **`transformers`**: The core Hugging Face library.\n",
        "- **`sentencepiece` & `sacremoses`**: Common tokenization dependencies.\n",
        "- **`datasets[audio]`**: This is a key command. It installs the `datasets` library from Hugging Face and includes the extra dependencies required for loading and processing audio datasets (like `libsndfile`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U transformers\n",
        "# !pip install -U sentencepiece\n",
        "# !pip install -U sacremoses\n",
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade transformers sentencepiece datasets[audio]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ 2. Setting a Custom Cache Directory (Optional)\n",
        "\n",
        "As with other models, Text-to-Speech models can be large. This commented-out block shows how you can set a custom cache directory for Hugging Face to store downloaded models, which is useful for managing disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# new_cache_dir = \"\"\"X:\\AI-learin\\courss\\Fine-Tuning-LLM-with-HuggingFace-main\\models\"\"\"\n",
        "# os.environ['HF_HOME'] = new_cache_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 3. Importing Necessary Libraries\n",
        "\n",
        "Here, we import all the tools we'll need for this task.\n",
        "\n",
        "- **`pipeline` from `transformers`**: Our main high-level API for inference.\n",
        "- **`load_dataset` from `datasets`**: A powerful function to download and use datasets directly from the Hugging Face Hub. We'll use this to get voice data.\n",
        "- **`soundfile` as `sf`**: A library for reading and writing audio files. We'll use it to save our generated speech as a `.wav` file.\n",
        "- **`torch`**: The PyTorch library. The underlying models run on PyTorch, and we'll use it to handle the numerical data for the voice embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 4. Defining the Input Text\n",
        "\n",
        "This is the text that we want our model to convert into spoken words. We define it here as a simple string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text=\"\"\"Sam Altman on Wednesday returned to OpenAI as the chief executive officer (CEO) and sacked the Board that had fired him last week. However, the only remaining member in the Board team is Adam D'Angelo, CEO of Quora.\\nEx-Salesforce co-CEO Bret Taylor and former US Treasury Secretary and president of Harvard University, Larry Summers will join D'Angelo.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó£Ô∏è 5. Performing Text-to-Speech Synthesis\n",
        "\n",
        "This is the core of the notebook where we generate speech from our text. This process has a few fascinating steps.\n",
        "\n",
        "1.  **Create the Pipeline**: We create a `pipeline` for **`\"text-to-speech\"`** using the **`\"microsoft/speecht5_tts\"`** model. We must set **`trust_remote_code=True`** because this model requires custom code from its repository to function correctly. This is a security measure, and you should only use it with models from trusted sources.\n",
        "\n",
        "2.  **Load a Speaker's Voice (Embedding)**: To control the voice of the generated speech, we don't just use the model alone. We load a dataset of **speaker embeddings** (also known as x-vectors). Think of these as numerical \"fingerprints\" that represent the unique characteristics of a person's voice.\n",
        "\n",
        "3.  **Select a Specific Voice**: From the thousands of voices in the dataset, we pick one (at index `7306`). We convert this voice fingerprint into a `torch.tensor` and use `.unsqueeze(0)` to format it correctly for the model.\n",
        "\n",
        "4.  **Synthesize the Speech**: We call our `synthesiser` with the input `text`. Critically, we pass our chosen `speaker_embedding` to the model using `forward_params`. This tells the model to generate the speech using the characteristics of that specific voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = \"microsoft/speecht5_tts\"\n",
        "synthesiser = pipeline(\"text-to-speech\", model= model, trust_remote_code=True)\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "speech = synthesiser( text, forward_params={\"speaker_embeddings\": speaker_embedding})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ 6. Saving the Generated Audio\n",
        "\n",
        "The output from the pipeline is a dictionary containing the raw audio data (as a NumPy array) and the correct sampling rate.\n",
        "\n",
        "We use the `soundfile.write()` function to save this data as a standard `.wav` file. We provide a filename (`\"speech.wav\"`), the audio data (`speech[\"audio\"]`), and the sampling rate (`speech[\"sampling_rate\"]`). This sampling rate is crucial for ensuring the audio plays back at the correct speed and pitch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
